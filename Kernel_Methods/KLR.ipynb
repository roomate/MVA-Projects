{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook contains an implementation of the Kernel Logistic Regression, a kernelization of the Linear Logistic Regression.\n",
    "After a few modelization step, the problem takes the form:\n",
    "$$\n",
    "\\min_{\\alpha \\in \\mathbb{R}^n} J(\\alpha) = \\frac{1}{n}\\sum_{i = 1}^n l_{\\text{log}}((K\\alpha)_i y_i) + \\lambda \\alpha^\\top K \\alpha\n",
    "$$\n",
    "where $l_{\\text{log}}(x) = \\frac{1}{1 + \\exp{-x}}$.\n",
    "Using a quadratic expansion, it reduces to a Weighter Kernel Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.35335283e-01, 1.35335283e-01],\n",
       "       [6.06530660e-01, 6.06530660e-01],\n",
       "       [1.26641655e-14, 1.26641655e-14],\n",
       "       [1.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def RBF(sigma = 1):\n",
    "    def kernel(x,y):\n",
    "        return np.exp(-np.sum((x - y)**2, axis = -1)/(2*sigma**2))\n",
    "    return kernel\n",
    "\n",
    "class WKernelRR:\n",
    "\n",
    "    def __init__(self,kernel,lmbda):\n",
    "        self.lmbda = lmbda\n",
    "        self.kernel = kernel\n",
    "        self.alpha = None \n",
    "        self.b = None\n",
    "        self.support = None\n",
    "        self.type='ridge'\n",
    "        self.weight = None #Is assumed to be a diagonal matrix with positive entries\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        N = X.shape[0] #Number of samples\n",
    "        X = X.reshape(N,-1)\n",
    "\n",
    "        self.support = X\n",
    "\n",
    "        K = self.kernel(X,X)\n",
    "        K_ridge = K + np.eye(N)*self.lmbda*N\n",
    "        inv_den = np.linalg.solve(K_ridge, np.ones(N))\n",
    "        inv_num = np.linalg.solve(K_ridge,y)\n",
    "        self.b = np.dot(np.sqrt(W),(N + np.sum(K @ inv_num) - np.sum(y))/(np.sum(K @ inv_den))\n",
    "        self.alpha = np.linalg.solve(K_ridge,y - self.b)\n",
    "\n",
    "    ### Implementation of the separting function $f$\n",
    "    def regression_function(self,x):\n",
    "        # Input : matrix x of shape N data points times d dimension\n",
    "        # Output: vector of size N\n",
    "        N = x.shape[0]\n",
    "        x = x.reshape(N,-1)\n",
    "        return self.kernel(x, self.support) @ self.alpha\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.regression_function(X) + self.b\n",
    "\n",
    "\n",
    "class KLR:\n",
    "    def __init__(self, X, label, kernel, lambda):\n",
    "        self.dim = X.shape[-1]\n",
    "        self.size = X.shape[0]\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "        self.kernel = kernel\n",
    "        self.lamb = lambda\n",
    "        self.K = None\n",
    "        self.alpha = None\n",
    "    \n",
    "    def init(self):\n",
    "        self.K = gram_matrix()\n",
    "\n",
    "    def gram_matrix(self):\n",
    "        return self.kernel(self.data[:,:,None], self.data[:,None,:])\n",
    "\n",
    "    def fit(self):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
